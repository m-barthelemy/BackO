//# define DEBUG
using System;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using System.Collections.Generic;
using System.Collections.Concurrent;
using System.Diagnostics;
using System.Security.Cryptography;
using Node.DataProcessing;
using Node.Utilities;
using P2PBackup.Common;

namespace Node{
	
	/// <summary>
	/// This class is responsible for getting backup chunks generated, and process them (transfer, maintain index),
	///  	following a Producer(Chunk.Build()) - Consumer(USer.SendPut+wait for transfer success) model
	/// It will process chunks building and transferring following the provided level of paralleism.
	/// Paralleism will directly impact memory usage:
	/// 	max used memory <=> parallelism*maxchunksize
	/// </summary>
	public class BackupManager{

		private Backup backup;
		private static List<BChunk> processingChunks; // manages chunks being processed 


		// *Producers data source
		//  MegaQueue will contains (n=parallelism) items, which are themselves queues containing backuprootdrives
		//  - if megaqueue elements are queues, each having  1 BackupRootderive only, parallelism is at FS level
		//  - if a megaqueue Queue items contains multiple BackupRootDrives, this means that we are parallelizing by physical disk (which makes more sense on most situations)
		private Queue<Queue<BackupRootDrive>> megaQueue;

		// *Consumers data source, generated by producers
		private BlockingCollection<BlockingCollection<BChunk>> chunksFeeds;

		// *Indexer data source, generated by consumers (chunks done and ready to be indexed)
		private BlockingCollection<BChunk> indexerChunksFeed; 

		private BChunk indexChunk;
		private int doneBdhProducers; // number of ran and terminated producers

		// Real parallelism (over asked by config parallelism) calculated after discovering the real number of drives(disks/fses)
		private int realParallelism;

		public delegate void BackupDoneHandler(long taskId);
		public event BackupDoneHandler BackupDoneEvent;

		// Interrupts the whole backup operation (producers & consumers), yet letting the index to be processed
		internal CancellationTokenSource cancellationTokenSource;

		IEnumerator<BackupRootDrive> brdEnumerator;

		private string syntheticIndexSum;
		// Maintain a list of currently running consumers (allow tracking to know if backup operation is complete)
		private List<System.Threading.Tasks.Task> consumers;

		/// <summary>
		/// Initializes a new instance of the <see cref="Node.BackupManager"/> class.
		/// Must be passed an initialized backup (with snapshots done ect) as parameter.
		/// </summary>
		/// <param name='b'>
		/// Backup intance previously created and prepared
		/// </param>
		/// <param name='taskId'>
		/// Task identifier.
		/// </param>
		public BackupManager(Backup b, long taskId){

			backup = b;
			int backingDevicesCount = 0;
			megaQueue = new Queue<Queue<BackupRootDrive>>();

			if(backup.Parallelism.Kind == ParallelismLevel.Disk){
				var backupNeededDisks = (from BackupRootDrive brd in backup.RootDrives
					select brd.SystemDrive.GetParentPhysicalDisks()[0]).Distinct();
				backingDevicesCount = backupNeededDisks.Count();
				Console.WriteLine ("  ----  backing devices:");
				foreach(P2PBackup.Common.Volumes.Disk d in backupNeededDisks.ToList()){
					//Console.WriteLine ("  ----      list");
					//foreach(P2PBackup.Common.Volumes.Disk d in l)
						Console.WriteLine ("  ----      "+d.ToString());
				}

				var fsesGroupedByDisk = backup.RootDrives.GroupBy(
					fs=>fs.SystemDrive.GetParentPhysicalDisks()[0].Path
				);

				foreach(var fsGroup in fsesGroupedByDisk){
					Queue<BackupRootDrive> diskFses = new Queue<BackupRootDrive>(fsGroup.ToList());
					megaQueue.Enqueue(diskFses);
				}

			}
			else if(backup.Parallelism.Kind == ParallelismLevel.FS){
				backingDevicesCount = backup.RootDrives.Count;
				foreach(BackupRootDrive brd in backup.RootDrives){
					Queue<BackupRootDrive> diskFses = new Queue<BackupRootDrive>();
					diskFses.Enqueue(brd);
					megaQueue.Enqueue(diskFses);
				}
			}
			doneBdhProducers = megaQueue.Count;
			if(backup.Parallelism.Value == 0){ // "auto" mode, set to the number of backing blockdevices (physical disk or partition)
				realParallelism = backingDevicesCount;
			}
			else // choose the lowest value, as it would make no sense to have a config // > physical number of drives(fs or disks) to process
				realParallelism = Math.Min((int)backup.Parallelism.Value, backingDevicesCount) ;

			Logger.Append(Severity.INFO, "Requested parallelism: "+this.backup.Parallelism.ToString()+", real: "+realParallelism);
			chunksFeeds = new BlockingCollection<BlockingCollection<BChunk>>(realParallelism+1); //+1 to allow feed to be re-added when consumer's budget is expired
			indexerChunksFeed = new BlockingCollection<BChunk>(new ConcurrentQueue<BChunk>(), realParallelism+1);

			processingChunks = new List<BChunk>();
			cancellationTokenSource = new CancellationTokenSource();

			consumers = new List<System.Threading.Tasks.Task>((int)b.Parallelism.Value);
		}
		
		~BackupManager(){
			Logger.Append(Severity.DEBUG2, "<TRACE> BackupManager destroyed.");	
		}
		
		internal void Run(){
			// at this stage we're ready to backup : we gathered/exploded/grouped backup paths, gathered necesary FSes 
			// and application objects. Thus if backup type is snapshotonly, we're done.
			if(backup.Level == BackupLevel.SnapshotOnly){

				ProcessIndex();
				return;

			}

			User.SendPut(backup.TaskId, -1, realParallelism);
			brdEnumerator = backup.RootDrives.GetEnumerator();
			User.StorageSessionReceivedEvent += new User.StorageSessionReceivedHandler(this.SessionReceived);

			for(uint i=0; i<realParallelism; i++){
				var producer = System.Threading.Tasks.Task.Factory.StartNew(()=>{
					Produce(megaQueue.Dequeue());
				}, TaskCreationOptions.LongRunning); // WRONG : split paths first
				producer.ContinueWith(o => UnexpectedError(producer), TaskContinuationOptions.OnlyOnFaulted);
			}

			// Start indexer task
			var indexer = System.Threading.Tasks.Task.Factory.StartNew(()=>{
					DoIndex();
			}, cancellationTokenSource.Token, TaskCreationOptions.None, TaskScheduler.Default);
			
			indexer.ContinueWith(o => UnexpectedError(indexer), TaskContinuationOptions.OnlyOnFaulted);
			indexer.ContinueWith(o => ProcessIndex(), TaskContinuationOptions.OnlyOnRanToCompletion /*| TaskContinuationOptions.NotOnFaulted| TaskContinuationOptions.NotOnCanceled*/);
			/*System.Threading.Tasks.Task.Factory.ContinueWhenAll(producers,  z=>{
					
			});*/
		}
		
		private void ContinueProducing(){

			Interlocked.Decrement(ref doneBdhProducers);
			if(megaQueue.Count == 0 && doneBdhProducers == 0){
				if(!cancellationTokenSource.IsCancellationRequested)
					Logger.Append(Severity.DEBUG, "Enough producers have been started");
					//Logger.Append(Severity.INFO, "Done gathering items to backup.");
				//chunksFeeds.CompleteAdding();
				return;
			}
			else{
				if(megaQueue.Count == 0) return; // no need to start a new producer
				Logger.Append(Severity.DEBUG, "Still "+megaQueue.Count+" producers to start, "+doneBdhProducers+" done");
				Queue<BackupRootDrive> nextQ = megaQueue.Dequeue();
				var continuedProducer = System.Threading.Tasks.Task.Factory.StartNew(() =>{
					Produce(nextQ);
				}, TaskCreationOptions.LongRunning);

			}

		}
		
		
		/// <summary>
		/// One 'Produce' task generates chunks for one BackupRootDrive (ie 1 mountpoint).
		/// </summary>
		/// <param name='bdr'>
		/// the BackupRootDrive to scan for items
		/// </param>
		private void Produce(Queue<BackupRootDrive> queue/*, BlockingCollection<BChunk> myFeed*/){

			BlockingCollection<BChunk> myFeed = new BlockingCollection<BChunk>(new ConcurrentQueue<BChunk>(), 1);
			Console.WriteLine ("    ------- Producer() has "+queue.Count+" drive items in its queue");
			chunksFeeds.Add(myFeed);
			//IEnumerator<BChunk> chunkEnumerator = bdh.GetNextChunk().GetEnumerator();//backup.GetNextChunk().GetEnumerator();
			while(queue.Count>0){
				BackupRootDrive bdr = queue.Dequeue();
				Logger.Append(Severity.INFO, "Collecting items to backup for drive "+bdr.SystemDrive.MountPoint);
				BackupRootDriveHandler bdh = new BackupRootDriveHandler(bdr, this.backup.TaskId, backup.MaxChunkSize, backup.MaxChunkSize, backup.MaxChunkFiles, backup.Level, backup.RefStartDate, backup.RefEndDate, backup.RefTaskId);
				bdh.LogEvent += LogReceived;
				bdh.SubCompletionEvent += new BackupRootDriveHandler.SubCompletionHandler(IncrementSubCompletion);
				foreach(P2PBackup.Common.BasePath baseP in bdr.Paths){
					bdh.SetCurrentPath(baseP);
					IEnumerator<BChunk> chunkEnumerator = bdh.GetNextChunk().GetEnumerator();
					while(chunkEnumerator.MoveNext() && !cancellationTokenSource.IsCancellationRequested){
						BChunk chunk = chunkEnumerator.Current;
						try{
							myFeed.Add(chunk, cancellationTokenSource.Token);
						}
						catch(OperationCanceledException){
							Logger.Append(Severity.DEBUG2, "Producer has been manually cancelled on purpose, stopping...");
							return;
						}
						catch(Exception e){
							Logger.Append(Severity.ERROR, "###################### Produce()	: add refused : "+e.Message+" ---- "+e.StackTrace);
							return;
						}
						// stats
						foreach(IFSEntry item in chunk.Files)
							backup.ItemsByType[(int)item.Kind]++;
						Logger.Append(Severity.DEBUG, "Basepath "+baseP.Path+" : Added chunk "+chunk.Name+" containing "+chunk.Files.Count+" items ");
					}
				}
				bdh.SubCompletionEvent -= new BackupRootDriveHandler.SubCompletionHandler(IncrementSubCompletion);
				bdh.LogEvent -= LogReceived;
				if(!cancellationTokenSource.IsCancellationRequested)
					ContinueProducing();
				else 
					bdh.Dispose();
			}
			Console.WriteLine ("------------------------- PRODUCE(): done collecting ALL, complete feed adding, cancellationTokenSource.IsCancellationRequested="+cancellationTokenSource.IsCancellationRequested);
			myFeed.CompleteAdding();
		}


		private void SessionReceived(long taskId, Session session/*, int budget*/){

			if(chunksFeeds.IsCompleted) return; // TODO : verify that in this case, session is closed.

			System.Threading.Tasks.Task consumeTask = System.Threading.Tasks.Task.Factory.StartNew(() =>{
					Consume(session);
			}, TaskCreationOptions.LongRunning);
			consumeTask.ContinueWith(o=>AfterTask(consumeTask, session), TaskContinuationOptions.OnlyOnRanToCompletion
			                         	| TaskContinuationOptions.NotOnFaulted | TaskContinuationOptions.NotOnCanceled);
			consumeTask.ContinueWith(o=>UnexpectedError(consumeTask), TaskContinuationOptions.OnlyOnFaulted);
			consumeTask.ContinueWith(o=>Cancel(), TaskContinuationOptions.OnlyOnCanceled);
			consumers.Add(consumeTask);
		}
		
		// TODO : split this into a dedicated consumer class, also merge ChunkProcessor in this future class
		// using a class will allow to track what the consumer is doing, and cancel it if it blocks on an empty chunksFeeds.Take (when everything has yet been processed)
		private void Consume(Session s/*, int budget*/){

			if(/*chunksFeeds.IsCompleted*/megaQueue.Count == 0 && doneBdhProducers == 0 && chunksFeeds.Count == 0){
				Console.WriteLine ("------------------------- CONSUME() : procducers queues marked complete and already processed, exiting");
				return;
			}

			// Filter client-side processing flags
			DataProcessingFlags clientFlags = DataProcessingFlags.None;
			foreach (DataProcessingFlags flag in Enum.GetValues(typeof(DataProcessingFlags))){
				if((int)flag < 512 && backup.DataFlags.HasFlag(flag))
					clientFlags |= flag;
			}
			
			DataPipeline pipeline = new DataPipeline(PipelineMode.Write, clientFlags, this.backup.Bs.Id);
			if(backup.DataFlags.HasFlag(DataProcessingFlags.CDedup))
			   pipeline.StorageNode = s.ClientId;
			if(backup.DataFlags.HasFlag(DataProcessingFlags.CEncrypt)){
				//X509Certificate2 cert = new X509Certificate2(ConfigManager.GetValue("Security.CertificateFile"), "");
			   //pipeline.CryptoKey = (RSACryptoServiceProvider)cert.PublicKey.Key;
				pipeline.CryptoKey = s.CryptoKey;

				byte[] iv = new byte[16];
				Array.Copy (System.BitConverter.GetBytes(backup.TaskId), iv, 8);
				Array.Copy (System.BitConverter.GetBytes(backup.TaskId), 0, iv, 8, 8);
				pipeline.IV = iv; //new byte[]{Convert.ToByte(backup.TaskId)};
			}
			if(chunksFeeds.Count == 0) return;
			ChunkProcessor cp = new ChunkProcessor(s, pipeline, backup);
			s.TransfertDoneEvent += new Session.TransfertDoneHandler(ManageChunkSent);
			Console.WriteLine ("------------------------- Consume("+s.Id+") : taking chunks feed from queue which has "+chunksFeeds.Count+" elements yet to processe");
			BlockingCollection<BChunk> myFeed = chunksFeeds.Take(this.cancellationTokenSource.Token);
			Console.WriteLine ("------------------------- Consume("+s.Id+") : got a new queue to process, yummy!");

			// We transfer chunks until reaching budget or there is no more chunks to send (backup done, or severe error)
			while((!myFeed.IsCompleted)  && (s.Budget >0)){
				if(cancellationTokenSource.IsCancellationRequested){
					s.LoggerInstance.Log(Severity.INFO, "Received cancellation request for task #"+backup.TaskId+", stop processing...");
					s.TransfertDoneEvent -= new Session.TransfertDoneHandler(ManageChunkSent);
					s.SendDisconnect();
					s.Disconnect();
					return;
				}
				BChunk chunk = null;
				try{

					chunk = myFeed.Take(cancellationTokenSource.Token);

					s.LoggerInstance.Log(Severity.DEBUG2, "Processing chunk "+chunk.Name);
					lock(processingChunks){
						processingChunks.Add(chunk);
					}
					//if(chunk.OriginalSize >0) // avoid to process empty chunks, as it complicates design on the receiving side(first read() neveir receiving data, thus blocking)
						cp.Process(chunk, backup.MaxChunkSize);
					//else{
					//	s.LoggerInstance.Log(Severity.DEBUG2, "Not sending chunk '"+chunk.Name+"' with null size");
					//	ManageChunkSent(true, backup.TaskId, chunk.Name, 0, 0);
					//}
					/*backup.OriginalSize += chunk.OriginalSize;
					backup.FinalSize += chunk.Size;
					backup.TotalChunks ++;
					backup.TotalItems += chunk.Files.Count;*/
					//if(chunk.Size > pipeline.HeaderLength)// an empty chunk doesn't count
					//		budget--;
					

					//if (chunk.SentEvent.Wait(cancellationTokenSource.Token)){ // (60000, false)){
					chunk.SentEvent.Wait(60000, cancellationTokenSource.Token);
						s.LoggerInstance.Log(Severity.DEBUG2, "Processed  chunk "+chunk.Name+", remaining budget="+s.Budget);
						chunk.SentEvent.Dispose();
					//}
					/*else{ // timeout waiting for storage node confirmation
						Logger.Append(Severity.WARNING, "Timeout waiting for storage node #"+s.ClientId+" ("+s.ClientIp+") confirmation, chunk "+chunk.Name);
						// TODO : but if have an error with one chunk, it's likely we will have one with next chunks too.
						//		close session now instead of continuing???
						try{
							//chunksFeed.Add(chunk, cancellationTokenSource.Token);
							myFeed.Add(chunk, cancellationTokenSource.Token);
						}
						catch(InvalidOperationException){
							Logger.Append(Severity.ERROR, "Timeout waiting for storage node #"+s.ClientId+" : A session error occured, unable to use a new session to process chunk (queue is closed)");	
							backup.AddHubNotificationEvent(811, chunk.Name, "Timeout waiting for storage node #"+s.ClientId+" : A session error occured, unable to use a new session to process chunk (queue is closed)");	
						}
					}*/
				}
				catch(System.Net.Sockets.SocketException e){
					// error sending to storage node. Re-add chunk to list and ask another storage session to hub.
					Console.WriteLine("############## Produce()	: TAKE refused for chunk "+chunk.Name+": "+e.Message+" ---- "+e.StackTrace);
					backup.AddHubNotificationEvent(811, chunk.Name, e.Message);
					if(chunk == null) return;
					RemoveChunk(chunk);
					//s.Disconnect();
					try{
						User.AskAlternateDestination(backup.TaskId, s.ClientId);
						//chunksFeed.Add(chunk, cancellationTokenSource.Token);
						myFeed.Add(chunk, cancellationTokenSource.Token);
					}
					catch(InvalidOperationException ioe){
						Logger.Append(Severity.ERROR, "A session error occured, unable to use a new session to process chunk (queue is closed) : "+ioe.Message);	
						backup.AddHubNotificationEvent(811, chunk.Name, "A session error occured, unable to use a new session to process chunk (queue is closed)");	
					}
					//throw new Exception("Something went wrong with this consumer");
				}
				catch(OperationCanceledException){
					Logger.Append(Severity.DEBUG2, "Consumer task has been manually cancelled on purpose, stopping...");
					s.TransfertDoneEvent -= new Session.TransfertDoneHandler(ManageChunkSent);
					return;
				}
				catch(InvalidOperationException){
					Logger.Append(Severity.DEBUG, "Consumer task has done processing its chunks list");
					             
				}
				/*Logger.Append(Severity.INFO, "DataProcessorStreams statistics : checksum="+BenchmarkStats.Instance().ChecksumTime
				              +"ms, dedup="+BenchmarkStats.Instance().DedupTime
				              +"ms, compress="+BenchmarkStats.Instance().CompressTime
							  +"ms, send="+BenchmarkStats.Instance().SendTime+"ms.");*/
			}
			if(!myFeed.IsCompleted){
				// re-add this non-terminated chunk list on queue, to be processed by next session.
				Console.WriteLine ("    ----------  Consumer : feed not completed, re-adding to chunksFeeds");
				chunksFeeds.Add(myFeed, cancellationTokenSource.Token);
				Console.WriteLine ("    ----------  Consumer : feed not completed, re-added, chunksFeeds count="+chunksFeeds.Count);

			}
			s.TransfertDoneEvent -= new Session.TransfertDoneHandler(ManageChunkSent);
			Logger.Append(Severity.DEBUG, "Session #"+s.Id+" with node #"+s.ClientId+": processed and transferred all data chunks, unused budget="+s.Budget);
		}


		private void AfterTask(System.Threading.Tasks.Task task, Session s){
		
			if(chunksFeeds.Count >0 && ! cancellationTokenSource.IsCancellationRequested){
				Logger.Append(Severity.DEBUG, "---------------------------------Asking for another storage session because of expired budget with storage node #"+s.ClientId+", current chunks feed queue count : "+chunksFeeds.Count);
				User.SendPut(backup.TaskId, s.Id, 1);
				return;
			}

			//unregister storage session for normal chunks
			s.TransfertDoneEvent -= new Session.TransfertDoneHandler(ManageChunkSent);
			// gracefully disconnect from storage node
			s.SendDisconnect();

			//task.Dispose();
			lock(consumers){ // not frequently called, so a global lock is perfectly fine here
				Console.WriteLine ("################### locked consumers");
				for(int i= consumers.Count-1; i>=0; i--){
					System.Threading.Tasks.Task t = consumers[i];
					Console.WriteLine ("################### inspecting task "+t.Id+", completed="+t.IsCompleted+", faulted="+t.IsFaulted+", cancelled="+t.IsCanceled);
					if(t.IsCanceled || t.IsCompleted){
						consumers[i].Dispose();
						consumers.RemoveAt(i);
					}
				}
			}
			Console.WriteLine ("###################1 chunksFeeds.Count =="+chunksFeeds.Count+", consumers.Count == "+consumers.Count);
			if(chunksFeeds.Count ==0 && consumers.Count == 0   /*  &&chunksFeeds.IsCompleted*/){
				indexerChunksFeed.CompleteAdding();
				Logger.Append(Severity.DEBUG, "Data backup done, ready to process index.");
			}
			else
				Console.WriteLine ("###################2 chunksFeeds.Count =="+chunksFeeds.Count+", consumers.Count == "+consumers.Count);

		}

		private void UnexpectedError(System.Threading.Tasks.Task task){
			var aggException = task.Exception.Flatten();
         	foreach(var exception in aggException.InnerExceptions){
				Logger.Append(Severity.CRITICAL, "Unexpected error while processing backup task "+this.backup.TaskId+" : "+exception.ToString());
				backup.AddHubNotificationEvent(999, exception.Message, "");
			}
			cancellationTokenSource.Cancel();
			//theSession.Disconnect();
			task.Dispose();
			//backup.Terminate(false);

			// rethrow to allow continuations to NOT be processed when they are NotOnFaulted
			throw new Exception("Propagating unexpected exception..."); 
			
		}

		private void ManageChunkSent(bool sent, long taskId, string chunkName, int destinationNode, int finalSize){

			if(!sent){
				Logger.Append(Severity.ERROR, "Chunk "+chunkName+" not sent - TODO : handle that");
				backup.AddHubNotificationEvent(999, "Chunk "+chunkName+" not sent - TODO : handle that", "");
				return;
			}
			lock(processingChunks){
				for(int i = processingChunks.Count-1; i>=0; i--){
					if(processingChunks[i].Name == chunkName){
						processingChunks[i].AddDestination(destinationNode);
						processingChunks[i].Size = finalSize;
						processingChunks[i].SentEvent.Set();
						backup.OriginalSize += processingChunks[i].OriginalSize;
						backup.FinalSize += processingChunks[i].Size;
						backup.TotalChunks ++;
						backup.TotalItems += processingChunks[i].Files.Count;
						
						indexerChunksFeed.Add(processingChunks[i], cancellationTokenSource.Token);
						processingChunks.RemoveAt(i);
						
						Logger.Append (Severity.DEBUG2, "Sent and indexed chunk "+chunkName);
					}
				}
				// to remove?
				//Thread.MemoryBarrier();
				if(chunksFeeds.IsCompleted && processingChunks.Count == 0 /*&& indexerChunksFeed.Count ==0*/){

					Logger.Append (Severity.INFO, "/// done backuping and indexing data");
					indexerChunksFeed.CompleteAdding();
				}
				/*else if(cancellationTokenSource.IsCancellationRequested)
					indexerChunksFeed.CompleteAdding();*/
				else {
					Logger.Append (Severity.INFO, " //// NOT done backuping and indexing data : processing count="+processingChunks.Count
					               +",indexerChunksFeed.Count="+indexerChunksFeed.Count+",chunkBuilderFeed.IsCompleted="+chunksFeeds.IsCompleted);
				}
			}


		}
		
		private void DoIndex(){
			while( !indexerChunksFeed.IsCompleted /*|| !cancellationTokenSource.IsCancellationRequested*/){
				try{
					BChunk toBeIndexed = indexerChunksFeed.Take(cancellationTokenSource.Token);
					backup.Index.AddChunk(toBeIndexed);
					Logger.Append(Severity.DEBUG2, "Added chunk "+toBeIndexed.Name+" to index");
				}
				catch(Exception e){
					if(e is OperationCanceledException)
						Logger.Append(Severity.DEBUG2, "Indexer has been manually cancelled on purpose, stopping...");
					else if(e is InvalidOperationException)
						Logger.Append(Severity.DEBUG2, "Indexer : no more chunks to index");
					else{
						Console.WriteLine ("////// unexpected DoIndex exception : "+e.ToString());
						throw;
					}
					return;
				}
			}
		}


		private void ProcessIndex(){
			//if(cancellationTokenSource.IsCancellationRequested) return;
				
			//indexProcessing = true;
			if(backup.Level != BackupLevel.SnapshotOnly && backup.DataFlags.HasFlag(DataProcessingFlags.CDedup)
			   /*	&& !cancellationTokenSource.IsCancellationRequested*/){ 
				// save dedup and process index even if task is cancelled (for cleaning purposes)
					try{
						DedupIndex.Instance().Persist();
					}
					catch(Exception _e){
						Logger.Append(Severity.ERROR, "Could not save deduplication indexes DB, backup data is therefore invalid. TODO: Report!!! : "+_e.Message+" ---- "+_e.StackTrace);
						backup.AddHubNotificationEvent(809, DedupIndex.Instance().IndexDBName,_e.Message);
					}
			}
			// now we have to send backup index and dedup index
			backup.Index.Terminate();
			//backup.Index = null;
			indexChunk = BuildIndexChunk();
		}
		
		
		private BChunk BuildIndexChunk(){
			backup.AddHubNotificationEvent(704, "","");
			BChunk iChunk = null;
			Console.WriteLine ("BuildIndexChunk() : --- 1");

			string synthIndexFullPath = null;
			Console.WriteLine ("BuildIndexChunk() : 1.1");
			if(backup.Bs.ScheduleTimes[0].Level != P2PBackup.Common.BackupLevel.Full && backup.Bs.ScheduleTimes[0].Level != P2PBackup.Common.BackupLevel.SnapshotOnly){
				Console.WriteLine ("BuildIndexChunk() : 1.2");
				IndexManager idxManager = new IndexManager();
				Logger.Append(Severity.INFO, "Building synthetic full index...");
				Console.WriteLine ("BuildIndexChunk() : 1.3");
				synthIndexFullPath = idxManager.CreateSyntheticFullIndex(backup.RefTaskId, backup.TaskId, backup.RootDrives);
				Console.WriteLine ("BuildIndexChunk() : 1.4");
				backup.AddHubNotificationEvent(707, "","");
				syntheticIndexSum = IndexManager.CheckSumIndex(backup.TaskId, false); // for synthetic backups
			}
				
			//	indexFullPath = backup.Index.FullName;

			Console.WriteLine ("BuildIndexChunk() : 2");
			if(backup.Level == BackupLevel.SyntheticFull){ // backup the synth index
				iChunk = new BChunk(synthIndexFullPath, synthIndexFullPath, backup.TaskId);
				iChunk.Add(new MinimalFsItem(synthIndexFullPath));// minimalitem because we only care the data, not the rest
			}
			else{ // only backup the partial index
				iChunk = new BChunk(/*backup.TaskId, */backup.Index.FullName, backup.Index.FullName, backup.TaskId); //string name, int bsid, string bPath, string snapPath)
				iChunk.Add(new MinimalFsItem(backup.Index.FullName));// minimalitem because we only care the data, not the rest
			}

			Console.WriteLine ("BuildIndexChunk() : 3");
			try{
				//iChunk.Add(FileProvider.GetFile(index.FullName));


				if(backup.Level != BackupLevel.SnapshotOnly && backup.DataFlags.HasFlag(DataProcessingFlags.CDedup)) // backup the deduplication database
					iChunk.Add(ItemProvider.GetProvider().GetItemByPath(DedupIndex.Instance().IndexDBName));
				/*string sumHash;
				using(FileStream cksumFS = new FileStream(backup.Index.FullName, FileMode.Open, FileAccess.Read)){
					sumHash = BitConverter.ToString(SHA1.Create().ComputeHash(cksumFS));
					iChunk.Sum = sumHash;
				}*/
				Console.WriteLine ("BuildIndexChunk() : 4 (before cksum)");

				iChunk.Sum = IndexManager.CheckSumIndex(backup.TaskId, (backup.Level == BackupLevel.Refresh));
				if(backup.Level == BackupLevel.Full)
					syntheticIndexSum = iChunk.Sum; // for Fulls

				// stop waiting for sessions used for regular data transfer...
				User.StorageSessionReceivedEvent -= new User.StorageSessionReceivedHandler(this.SessionReceived);
				// ...but do register for session to perform index transfer
				User.StorageSessionReceivedEvent += new User.StorageSessionReceivedHandler(this.SendIndex);
				Console.WriteLine ("BuildIndexChunk() : 5");
				User.AskIndexDest(backup.TaskId, backup.Index.Name, iChunk.Sum);
				Logger.Append(Severity.DEBUG, "Asked index destination to hub");


				return iChunk;
			}
			catch(Exception e){
				Logger.Append(Severity.ERROR, "Couldn't checksum index and/or ask destination to hub: "+e.Message+"---"+e.StackTrace);
				backup.AddHubNotificationEvent(808, e.Message,"");
			}
			return null;
		}
		
		private void SendIndex(long taskId, Session s /*, int budget*/){
			System.Threading.Tasks.Task consumeIndexTask = System.Threading.Tasks.Task.Factory.StartNew(() =>{
					DataPipeline pipeline = new DataPipeline(PipelineMode.Write, DataProcessingFlags.CCompress|DataProcessingFlags.CChecksum);
					ChunkProcessor cp = new ChunkProcessor(s, pipeline, backup);
					cp.Process(indexChunk, backup.MaxChunkSize *10);
					indexChunk.Size = pipeline.Stream.Length;
					indexChunk.AddDestination(s.ClientId);
			}, TaskCreationOptions.LongRunning);

			consumeIndexTask.ContinueWith(o=>{
				Logger.Append(Severity.INFO, "Processed and sent backup index");
				backup.AddHubNotificationEvent(705, Math.Round((double)indexChunk.Size/1024/1024, 1).ToString(), "");

				User.SendDoneBackup(taskId, backup.OriginalSize, backup.FinalSize, backup.TotalItems, indexChunk.Name, indexChunk.Sum, syntheticIndexSum, indexChunk.StorageDestinations, 100);
				Logger.Append(Severity.INFO, "Task "+taskId+" has finished. "+backup.TotalItems+" items, "+backup.TotalChunks+" chunks. Original data size="+Math.Round((double)backup.OriginalSize/1024/1024,1)+"MB, final="+Math.Round((double)backup.FinalSize/1024/1024,1)+"MB");
				string statsByKind = "Task "+taskId+" processed: ";
				for(int i=0; i<10; i++)
				 	statsByKind += backup.ItemsByType[i]+" "+((FileType)i).ToString()+", ";
				Logger.Append(Severity.INFO, statsByKind);

#if DEBUG
				Logger.Append(Severity.INFO, "DataProcessorStreams statistics : checksum="+BenchmarkStats.Instance().ChecksumTime
				              +"ms, dedup="+BenchmarkStats.Instance().DedupTime
				              +"ms, compress="+BenchmarkStats.Instance().CompressTime
							  +"ms, send="+BenchmarkStats.Instance().SendTime+"ms.");
				Logger.Append(Severity.INFO, "Dedup statistics : lookups="+BenchmarkStats.Instance().DedupLookups
				              +", hotfound="+BenchmarkStats.Instance().DedupHotFound
				              +", coldfound="+BenchmarkStats.Instance().DedupColdFound
							  +", add="+BenchmarkStats.Instance().DedupAdd+".");
#endif

				User.StorageSessionReceivedEvent -= new User.StorageSessionReceivedHandler(this.SendIndex);

				backup.AddHubNotificationEvent(706, "","");
				backup.Terminate(true);

				BackupDoneEvent(taskId);
					
			}, TaskContinuationOptions.OnlyOnRanToCompletion
				|TaskContinuationOptions.ExecuteSynchronously
				|TaskContinuationOptions.NotOnFaulted
				|TaskContinuationOptions.NotOnCanceled
			);
			
			//consumeTask.Dispose();
		}
		
		
		private void IncrementSubCompletion(string newPath){
			backup.SubCompletion++;	
			backup.CurrentAction = newPath;
		}

		private void LogReceived(object sender, LogEventArgs args){
			backup.AddHubNotificationEvent(args.Code, args.Message, "");

		}

		private void RemoveChunk(BChunk chunkToRemove){
			lock(processingChunks){
				for(int i = processingChunks.Count-1; i>=0; i--)
					if(processingChunks[i].Name == chunkToRemove.Name)
						processingChunks.RemoveAt(i);
			}	
		}

		private void Cancel(){
			/*lock(consumers){
				if(consumers.Count == 0)
					indexerChunksFeed.CompleteAdding();
			}*/
			System.Threading.Tasks.Task.WaitAll(consumers.ToArray());
			indexerChunksFeed.CompleteAdding();
		}


	}

}
